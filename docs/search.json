[
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Florian Greindl\nDepartment of Economics\nUniversity of Regensburg\nUniversitätsstr. 31\nD-93053 Regensburg\nGermany"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome…",
    "section": "",
    "text": "…to my website, which is of no use to anyone (except me, who’s trying to make a website).\n$$\n$$"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Some Thoughts on Linear Regression and OLS\n\n\n\nMachine Learning\n\n\n\nA post that tries to wrap up stuff I know about neural networks in a concise way\n\n\n\nFlorian Greindl\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-03-18-neural_network/index.html",
    "href": "posts/2024-03-18-neural_network/index.html",
    "title": "Some Thoughts on Linear Regression and OLS",
    "section": "",
    "text": "The presented content is primarily based on a paper by Catherine F. Higham and Desmond J. Higham (Higham and Higham 2018), which presents a concise introduction to neural networks addressed at applied mathematicians."
  },
  {
    "objectID": "posts/2024-03-18-neural_network/index.html#structure-of-an-artificial-neural-network-nn",
    "href": "posts/2024-03-18-neural_network/index.html#structure-of-an-artificial-neural-network-nn",
    "title": "Some Thoughts on Linear Regression and OLS",
    "section": "Structure of an artificial neural network (NN)",
    "text": "Structure of an artificial neural network (NN)\nAn artificial NN consists of several layers. We denote the number of layers \\(L\\). Each layer consists of neurons. We write \\(n_l\\) for the number of neurons in layer \\(l\\in \\{1,\\dots,L\\}\\). The “state” of a neuron is given by its activation, which is a real number between \\(0\\) and \\(1\\). We denote the activation of neuron \\(i\\in \\{1,\\dots, n_l\\}\\) in layer \\(l\\) with \\(a^{[l]}_i\\in [0,1]\\). How is the activation of a neuron determined? Given some input \\(z\\in\\mathbb{R}\\) we can feed \\(z\\) into a function \\(\\sigma\\) that maps \\(z\\) to a number between 0 and 1. Traditionally a popular choice for \\(\\sigma\\) has been \\[\\sigma(z)=(1+\\exp(-z))^{-1},\\] which has some desireable properties:\n\nits derivative is easy to calculate as we have \\[\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\]\nand it is strictly increasing in \\(z\\) and its first derivative is largest at \\(0\\).\n\n\nfrom sympy import symbols, exp, plot_parametric\n\nfrom sympy.plotting import plot\nz = symbols(\"z\")\np = plot_parametric((z,1/(1+exp((-z)))), (z,-5,5), label = r'$\\sigma(z) =(1-\\exp(-z))^{-1}$', legend = True, xlabel = \"z\", ylabel=\"y\")\n\n\n\n\nNow, let \\(l\\in\\{1,\\dots,L-1\\}\\) and assume we feed an observation into the neural network. We consider how the layers \\(l\\) and \\(l+1\\) of a NN are interconnected. The \\(i\\)th neuron in layer \\(l+1\\) is “wired” with every neuron of the previous layer, i.e. it takes the activations \\(a^{[l]}_1,\\dots, a^{[l]}_{n_l}\\) as input. Then a weighted sum \\[\\sum_{j=1}^{n_l}w^{[l+1]}_{ij}a_j^{[l]}\\] of these inputs is calculated to which we add a bias \\(b_i^{[l]}\\). We write \\[z_j^{[l+1]}=\\sum_{j=1}^{n_l}w^{[l+1]}_{ij}a_j^{[l]}+b_i^{[l+1]}\\] for \\(l\\in\\{1,\\dots,L-1\\}\\). To determine the activation of the neuron we calculate \\(\\sigma(z_j^{[l]})\\in[0,1]\\), which gives us \\(a_j^{[l]}\\) . We do this for every neuron in layer \\(l\\). Hence, we can collect the weights into a \\(n_{l+1}\\times n_l\\) matrix \\(W^{[l+1]}\\). The shape stems from the fact that (a) the input each neuron takes are \\(n_l\\) real numbers between 0 and 1 and (b) we have \\(n_{l+1}\\) neurons in layer \\(l+1\\) that accept these inputs. Analogously we write \\(b^{[l+1]}\\in\\mathbb{R}^{n_{l+1}}\\) for the vector of biases. The entry \\(w^{[l+1]}_{ij}\\) determines how much neuron \\(j\\) in layer \\(l\\) influences neuron \\(i\\) in layer \\(l+1\\). We write \\[z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l]}\\] and\n\\[a^{[l+1]}=\\sigma(z^{[l+1]})=(\\sigma(z_j^{[l]}))_{j=1,\\dots,n_l}\\] for the activations in layer \\(l\\)\nThere are two distinct layers, the first and the last one. The first layer aka the initial layer is fed with observed data. The final layer is the layer that gives us predictions. Let us consider the standard example of classifying hand-written digits. We assume that we have grey-scale pictures as input. Each pixel’s coloring can be represented by a number between 0 and 1. The number of pixels of each picture gives us the number of neurons \\(n_1\\) in the first layer. The intitial layer neurons’ activations are given by its grey scale value. We want the final layer to predict which digit was on the image fed into the NN. Hence, we have \\(n_L=10\\), where each neuron represents a digit. Our prediction is given by the neuron with the highest activation.\nWe can interpret a NN as a function \\[F:\\mathbb{R}^{n_1}\\rightarrow\\mathbb{R}^{n_L},\\] which takes vectors \\((x_1,\\dots,x_{n_1})\\) as input and puts out a vector of activations \\((a^{[L]}_1,\\dots,a^{[L]}_{n_L})\\). The function \\(F\\) itself is determined by the weights and biases on each layer.\nLet us consider a neural network with 4 layers and \\(n_1=2, n_2=2, n_3=3, n_4=2\\). Then \\(W^{[2]}\\) is a \\(2\\times 2\\) , \\(W^{[3]}\\) a \\(3\\times 2\\) and \\(W^{[4]}\\) a \\(2\\times 3\\) matrix. Thus, we have 16 weights. Additionally, we have \\(2+3+2=7\\) biases. Hence, by choosing \\(16+7=23\\) parameters we define a function \\(F:\\mathbb{R}^2\\rightarrow \\mathbb{R}^2.\\) Training this NN basically means adjusting these 23 parameters such that a cost function is minimized. It is already somewhat clear that it’s not gonna be an easy task to find optimal even for this baby NN.\nTo sum up this paragraph we provide pseudo-code that describes how an observation \\(x\\in\\mathbb{R}^{n_1}\\) is processed by the NN:\na = x\nfor k in 2 to L:\n  z = W_k*a + b_k\n  a = sigma(z)"
  },
  {
    "objectID": "posts/2024-03-18-neural_network/index.html#evaluating-our-predictions",
    "href": "posts/2024-03-18-neural_network/index.html#evaluating-our-predictions",
    "title": "Some Thoughts on Linear Regression and OLS",
    "section": "Evaluating our Predictions",
    "text": "Evaluating our Predictions\nLet us assume we want to classify observations into categories based on \\(n_1\\) observable characteristics. Let \\(N\\) be the number of observations in our labeled training data. Let’s assume we have already chosen weights and biases within our NN. Then, we can feed the observations into the calibrated NN. For every observation \\(x^{\\{i\\}}\\) the NN puts out an \\(n_L\\) dimensional vector representing the activations of the neurons in the final layer. As mentioned before, each neuron in the final layer represents a category. For every observation we know its category. Let us assume observation \\(i\\) belongs to category \\(j\\in\\{1,\\dots,n_L\\}\\). In an ideal world our NN’s output is \\(F(x^{\\{i\\}})=\\delta_{ij}\\) (Kronecker-delta), i.e. the activation of neuron \\(j\\) is 1, whilst the activations of the other neurons are 0. We can quantify our NNs deviation form the perfect classifier by calculating \\[C_i=\\frac{1}{2}||y^{\\{i\\}}-F(x^{\\{i\\}})||_2^2\\] for each \\(i\\in \\{1,\\dots,N\\}\\) and computing \\[N^{-1}\\sum_{i=1}^N C_i.\\] This defines our cost function. As this sum depends on the underlying parameters of \\(F\\), which we denote (for the sake of compactness) with \\(p\\), we can write \\[Cost(p)=N^{-1}\\sum_{i=1}^N C_i(p).\\] Our objective is to find parameters \\(p\\) such that this sum is minimal.\nWe continue the pseudo-code example:\nCost = 0\nfor i in 1 to N:\n  a = x _i\n  y = y_i\n  # feed observation to neural net\n  for k in 2 to L: \n    z = W_k*a + b_k \n    a = sigma(z)\n  C_i = 0.5 * sqrt((y[1]-a[1])^2+...+(y[n_L]-a[n_l])^2  \n  Cost = Cost + (1/N) *C_i"
  },
  {
    "objectID": "posts/2024-03-18-neural_network/index.html#stochastic-gradient",
    "href": "posts/2024-03-18-neural_network/index.html#stochastic-gradient",
    "title": "Some Thoughts on Linear Regression and OLS",
    "section": "Stochastic Gradient",
    "text": "Stochastic Gradient\n\nChoosing a direction\nLet us assume we have started with an initial calibration of our NN. The calibration is given by the parameter vector \\(p\\in\\mathbb{R}^s\\). Based on this we have calculated \\(Cost(p)\\). Now, we try to change the parameters by \\(\\Delta p\\) such that \\(Cost(p+\\Delta p)&lt; Cost(p)\\). Geometrically speaking, we want to find the fastest way downhill in a \\(s+1\\) dimensional mountain range. From multivariate calculus we know that \\[Cost(p+\\Delta p)\\approx Cost(p)+(\\nabla Cost(p),\\Delta p)_2\\] close to \\(p\\) (first-order Taylor approximation). One might also recall that \\(\\nabla Cost(p)\\) gives us the direction of the steepest ascent. Hence, we decide to go into the opposite direction, i.e. we choose \\(\\Delta p=-\\nabla Cost(p)\\). As the Taylor formula gives only valid approximations close to \\(p\\) we should not walk to far, i.e. we fix a scalar \\(\\eta\\) that we premultiply \\(\\Delta p\\) with. Hence, we update \\(p\\) as follows: \\[p\\rightarrow p-\\eta \\nabla Cost(p)\\] We do this until a stopping criterion is met.\n\n\nChoosing a Direction “stochastically”\nRemember that \\[Cost(p)=N^{-1}\\sum_{i=1}^N C_i(p).\\] Hence, \\[\\nabla Cost(p)=N^{-1}\\sum_{i=1}^N \\nabla C_i(p).\\] To calculate \\(\\nabla Cost(p)\\) once we have to calculate \\(s\\) (remember that \\(p\\in\\mathbb{R}^s\\)) partial derivatives \\(N\\) times. We have seen before that \\(s\\) is large even for a baby NN. Thus, calculationg \\(\\nabla Cost(p)\\) can be computationally costly. Consequently, we might only consider a subset of our training data in determining the next direction we walk to. For the sake of simplicity we pick only one observation \\(i\\) randomly, calculate \\(\\nabla C_i(p)\\) and walk into this direction. A single step can be summarized as\n\nChoose \\(i\\in \\{1,\\dots,N\\}\\) randomly.\nUpdate \\(p\\rightarrow p-\\eta \\nabla C_i(p)\\).\n\nWe call this the stochastic gradient method. Obviously to base our step only on one observation is not necessarily the most intelligent approach. One often uses mini-batches of the training data to update the parameter vector."
  },
  {
    "objectID": "posts/2024-03-18-neural_network/index.html#back-propagation",
    "href": "posts/2024-03-18-neural_network/index.html#back-propagation",
    "title": "Some Thoughts on Linear Regression and OLS",
    "section": "Back propagation",
    "text": "Back propagation\nThis section seeks a way to calculate \\(\\nabla C_i(p)\\). Let us write \\[C=\\frac{1}{2}||y-a^{[L]}||_2^2.\\] \\(C\\) is a function of the weights and biases via \\(a^{[L]}\\). Remember that the levels of activation of the neurons are calculated recursively, i.e. \\[a^{[L]}=\\sigma(W^{[L]} a^{[L-1]}+b^{[L]})=\\sigma (W^{[L]} \\sigma(W^{[L-1]} a^{[L-2]}+b^{[L-1]})+b^{[L]})=....\\]\nThus, we should spend some time to figure out how to calculate \\[\\frac{\\partial C}{\\partial w_{j,k}^{[l]}}\\] and \\[\\frac{\\partial C}{\\partial b_{j}^{[l]}}\\] for \\(l\\in\\{2,\\dots,L\\}\\) and \\(j=1,\\dots,n_l, k=1,\\dots,n_{l-1}\\). Above we have defined \\[z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\] and \\(a^{]l]}=\\sigma(z^{[l]})\\). We define \\[\\delta_j^{[l]}=\\frac{\\partial C}{\\partial z_j^{[l]} }\\] for \\(1\\leq j\\leq n_l\\) and \\(2\\leq l\\leq L\\). This measures how sensitive the cost function is to changes to the weighted and biased input of neuron \\(j\\) in layer \\(l\\) of the NN. This \\(\\delta\\)’s come in handy, when we try to find concise expressions for the partial derivatives of the cost function with respect to weights and biases.\n\nLemma\n\nWe have \\begin{align}\\delta^{[L]}&=\\sigma’(z^{[L]})\\circ (a^L -y),\\\\ \\delta^{[l]} &=\\sigma’(z^{[l]})\\circ (W^{[l+1]})^T \\delta^{[l+1]}\\text{ for }2\\leq l\\leq L-1\\\\ \\frac{\\partial C}{\\partial b_j^{[l]}} &= \\delta_j^{[l]} \\text{ for } 2\\leq l\\leq L\\\\ \\frac{\\partial C}{\\partial w_{jk}^{[l]}} &= \\delta_j^{[l]}a_k^{[l-1]} \\text{ for } 2\\leq l\\leq L\\end{align}, where \\(x\\circ y=(x_1y_1,\\dots,x_ny_n)^T\\) is the Hadamard-product."
  }
]