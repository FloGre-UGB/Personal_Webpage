<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Florian Greindl">
<meta name="dcterms.date" content="2022-10-24">
<meta name="description" content="A post that tries to wrap up stuff I know about OLS in a concise way">

<title>Florian Greindl - Some Thoughts on Linear Regression and OLS</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Florian Greindl</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../contact.html" rel="" target="">
 <span class="menu-text">Contact</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html" rel="" target="">
 <span class="menu-text">My Blog</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
    <a href="https://github.com/FloGre-UGB" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation-for-ols" id="toc-motivation-for-ols" class="nav-link active" data-scroll-target="#motivation-for-ols">Motivation for OLS</a>
  <ul class="collapse">
  <li><a href="#minimum-mean-square-error-predictor" id="toc-minimum-mean-square-error-predictor" class="nav-link" data-scroll-target="#minimum-mean-square-error-predictor">Minimum Mean Square Error Predictor</a></li>
  <li><a href="#back-to-the-conditional-expected-value" id="toc-back-to-the-conditional-expected-value" class="nav-link" data-scroll-target="#back-to-the-conditional-expected-value">Back to the conditional expected value</a></li>
  <li><a href="#modeling-the-conditional-expected-value" id="toc-modeling-the-conditional-expected-value" class="nav-link" data-scroll-target="#modeling-the-conditional-expected-value">Modeling the conditional expected value</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Some Thoughts on Linear Regression and OLS</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Econometrics</div>
  </div>
  </div>

<div>
  <div class="description">
    A post that tries to wrap up stuff I know about OLS in a concise way
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://florian-greindl.quarto.pub/">Florian Greindl</a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 24, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><em>Inspired by a course on Advanced Econometrics organized by the BGPE and taught by Jeffrey Wooldridge (Michigan State University) I decided to wrap up some basic knowledge on Econometrics that I have gathered over the course of my studies. The material presented is primarily based on unpublished lecture notes by Jeff Wooldridge, Wooldridge’s book “Introductory Econometrics”</em> <span class="citation" data-cites="wooldridge2009introductory">(<a href="#ref-wooldridge2009introductory" role="doc-biblioref">Wooldridge 2009</a>)</span> <em>and lecture notes by Rolf Tschernig (University of Regensburg)</em> <span class="citation" data-cites="tschernigkursmaterial">(<a href="#ref-tschernigkursmaterial" role="doc-biblioref">Tschernig and Haupt 2020</a>)</span><em>.</em></p>
<p>One might claim that there’s not much to say about linear regression and OLS. Nevertheless, I decided to write a post about these topics. In my experience we tend to view “beginner’s” material as straightforward and not worth thinking about. This includes myself. This post can be viewed as an attempt to prove myself wrong.</p>
<section id="motivation-for-ols" class="level2">
<h2 class="anchored" data-anchor-id="motivation-for-ols">Motivation for OLS</h2>
<p>Let us assume that we want to understand how a bunch of explanatory variables <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_p\)</span> influence an outcome variable <span class="math inline">\(y\)</span> in a population. Our objective is to make a good guess about <span class="math inline">\(y\)</span> if someone tells us the values of the explanatory variables for an individual <span class="math inline">\(i\)</span> in the population. A “good” guess might be the conditional expected value <span class="math display">\[E(y_i|x_{i1},\dots,x_{pi}).\]</span>While this is intuitively clear, we should remind ourselves which notion of good implies this choice.</p>
<section id="minimum-mean-square-error-predictor" class="level3">
<h3 class="anchored" data-anchor-id="minimum-mean-square-error-predictor">Minimum Mean Square Error Predictor</h3>
<p>Let’s take a step back and think about estimation in a more general setting. Assume we have a random variable <span class="math display">\[x:\Omega \rightarrow \mathbb{R}^n\]</span>with probability density function <span class="math inline">\(p^x\)</span>. In order to choose a good estimator for <span class="math inline">\(x\)</span> we define a cost-function, i.e.&nbsp;a function that penalizes the estimates deviation from the true <span class="math inline">\(x\)</span>. More precisely, we</p>
<ul>
<li><p>define a cost-function <span class="math display">\[c: \mathbb{R}^n \times \mathbb{R}^n  \rightarrow \mathbb{R}\]</span> and</p></li>
<li><p>pick an estimate <span class="math inline">\(\hat{x}\)</span> such that the expected cost is minimized, i.e.&nbsp;<span class="math inline">\(\hat{x}\)</span> minimizes <span class="math inline">\(E(c(x, \hat{x}))\)</span>.</p></li>
</ul>
<p>Let’s look at the cost function <span class="math display">\[c(x,\hat{x})=\|x-\hat{x}\|^2.\]</span> Then the <strong>mean squared error (MSE)</strong> is the expected cost given this cost function, i.e.&nbsp;<span class="math display">\[E(\|x-\hat{x}\|^2)=\int_\Omega \|x-\hat{x}\|^2 p^x(x) dx.\]</span></p>
<p>Now, let’s try to find the minimizer of this expression, i.e.&nbsp;<span class="math inline">\(\hat{x}\)</span> such that <span class="math inline">\(E(\|x-\hat{x}\|^2)\)</span> is minimal . We have <span class="math display">\[E(\|x-\hat{x}\|^2) = E(||x||^2)-2\hat{x}^T E(x) + \hat{x}^T\hat{x},\]</span> where we use that we can write <span class="math display">\[c(x,\hat{x})=\|x-\hat{x}\|^2=(x-\hat{x})^T (x-\hat{x}).\]</span></p>
<p>Note, that we have to assume that <span class="math inline">\(E(||x||^2)&lt;\infty\)</span>. Now, we differentiate with respect to <span class="math inline">\(\hat{x}\)</span> (one is often somewhat sloppy when it comes to this part of the argument, by simply assuming differentiability). Obviously the first term <span class="math inline">\(E(||x||^2)\)</span> vanishes when we differentiate with respect to <span class="math inline">\(\hat{x}\)</span>. Next, let us consider the third term and recap some multivariate calculus.</p>
<section id="some-multivariate-calculus" class="level4">
<h4 class="anchored" data-anchor-id="some-multivariate-calculus"><strong>Some multivariate calculus</strong></h4>
<p>Let us consider a billinear form <span class="math inline">\(\beta:Y_1\times Y_2 \rightarrow Z\)</span> where <span class="math inline">\(Y_1,Y_2,Z\)</span> are finite-dimensional normed vector spaces. Such a bilinear form is continuously differentiable and its derivative is <span class="math display">\[D\beta(a_1,a_2)(h_1,h_2)=\beta(h_1,a_2)+\beta(a_1,h_2).\]</span> This looks somehwat complicated, so let’s recall what a derivative actually is. One can define the mapping <span class="math inline">\(D\beta:Y_1\times Y_2\rightarrow \mathcal{L}(Y_1\times Y_2, Z)\)</span> where <span class="math inline">\(\mathcal{L}(Y_1\times Y_2,Z)\)</span> denotes the space of linear maps between <span class="math inline">\(Y_1\times Y_2\)</span> and <span class="math inline">\(Z\)</span>. This means: by choosing a point to differentiate at <span class="math inline">\((a_1,a_2)\)</span> we produce a linear map <span class="math inline">\(Y_1\times Y_2 \rightarrow Z\)</span>, which takes vectors in <span class="math inline">\((h_1,h_2)\)</span> in <span class="math inline">\(Y_1\times Y_2\)</span> as input.</p>
</section>
<section id="back-to-reality" class="level4">
<h4 class="anchored" data-anchor-id="back-to-reality">Back to “Reality”</h4>
<p>So what does this unnecessarily general result for bilinear form imply for the term <span class="math inline">\(\hat{x}^T \hat{x},\)</span> which we want to differentiate with respect to <span class="math inline">\(\hat{x}.\)</span> In our context <span class="math display">\[\beta=\langle\cdot, -\rangle:\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}.\]</span> By the fore mentioned result we know that <span class="math display">\[D\beta(\eta, \xi)(-,\cdot) = \langle\eta, \cdot\rangle + \langle -, \xi\rangle.\]</span> Thus, <span class="math display">\[D\beta(\hat{x},\hat{x})(-,\cdot)= \hat{x}^T \cdot + -^T \hat{x} = 2 x^T (\cdot + -),\]</span> where we use symmetry and linearity in the second component. To put it less confusingly <span class="math inline">\(D\beta(\hat{x},\hat{x})=2\hat{x}^T=2\hat{x},\)</span> where the equality holds in <span class="math inline">\(\mathcal{L}(Y_1\times Y_2,Z)\)</span>. In order to differentiate the second term we trust in mathematics and assume that everything works as we would expect it to work. Thus, we get <span class="math inline">\(E(x)\)</span> as the derivative of <span class="math inline">\(\hat{x}^T E(x)\)</span> with respect to <span class="math inline">\(\hat{x}\)</span> and we get <span class="math display">\[\frac{d}{d\hat{x}} E(\|x-\hat{x}\|^2)= -2E(x)+ 2x^T.\]</span></p>
<p>Setting this to zero and rearranging yields <span class="math display">\[\hat{x}=E(x),\]</span> which implies that the minimum mean square error predictor for <span class="math inline">\(x\)</span> is <span class="math inline">\(E(x)\)</span>.</p>
</section>
</section>
<section id="back-to-the-conditional-expected-value" class="level3">
<h3 class="anchored" data-anchor-id="back-to-the-conditional-expected-value">Back to the conditional expected value</h3>
<p>Let’s return to our actual topic, OLS. In the previous paragraph we saw that the idea of minimizing the mean squared error of our prediction leads us to considering estimating the conditional expectation <span class="math inline">\(E(y\mid x_1,\dots, x_p)\)</span>. But wait, I haven’t talked about conditional expectations in the previous paragraph. First of all, I should admit that my notation in the previous paragraph sucks as it is somewhat misleading. In our context we want to estimate <span class="math inline">\(y_i\)</span> based on information about the explanatory variables <span class="math inline">\(x_1,\dots, x_p\)</span> . As we have this additional information we seek to find the estimator that minimizes the conditional MSE <span class="math display">\[E(||y-\hat{y}||^2|x_{i1},\dots,x_{ip})=E(||y_i||^2|\cdot)-2 E(y_i|\cdot)\hat{y}+\hat{y}^2.\]</span> By taking the derivative wrt <span class="math inline">\(\hat{y}\)</span> and setting the result to zero we get <span class="math display">\[\hat{y}= E(y_i|x_{i1},\dots, x_{i_p}),\]</span></p>
<p>and we see that the conditional expected value is the minimum mean square error predictor for <span class="math inline">\(y_i\)</span> if we have information on the explanatory variables.<br>
</p>
</section>
<section id="modeling-the-conditional-expected-value" class="level3">
<h3 class="anchored" data-anchor-id="modeling-the-conditional-expected-value">Modeling the conditional expected value</h3>
<p>In many textbooks, one would now continue with the assumption that <span class="math display">\[E(y|x_1,\dots,x_p)=\beta_0+\beta_1 x_1+\dots+\beta_px_p.\]</span>Then, one would show that (under appropriate assumptions) OLS estimates the <span class="math inline">\(\beta\)</span>’s consistently. However, <span class="math inline">\(E(y|x_1,\dots,x_p)=\beta_0+\beta_1 x_1+\dots+\beta_px_p\)</span> is a fairly strong assumption (even if we allow for the <span class="math inline">\(x\)</span>’s to be transformations of variables). Why is this a strong assumption? Because we assume <strong>linearity in the parameters</strong>. Yes, you’ve just witnessed me dropping this phrase. But, what does this actually mean? Let’s use matrix notation to reduce writing effort (and have an expression that feels more like <strong>linear</strong> algebra): <span class="math display">\[E(y|x)=x^T\beta\]</span> Now, linearity in the parameters just means that <span class="math display">\[E(y|X)=L(X)\beta,\]</span> and <span class="math inline">\(L(X):\mathbb{R}^p\rightarrow \mathbb{R}, \beta \mapsto X\beta\)</span> is a linear map. People justifiably claim that this is not the case in many contexts. Nevertheless, even if we assume that our model for <span class="math inline">\(E(y|x_1,\dots,x_p)\)</span> is not true, OLS can still deliver interesting results.</p>
<section id="linear-projections" class="level4">
<h4 class="anchored" data-anchor-id="linear-projections">Linear Projections</h4>
<p>Let us assume that <span class="math inline">\(E(y|x_1,\dots,x_p)=\mu(x),\)</span> where <span class="math inline">\(\mu\)</span> can basically be anything. We have seen that <span class="math inline">\(\mu(x)\)</span> is the predictor that minimizes the mean squared error, i.e.&nbsp;<span class="math display">\[\mu(x)=\text{argmin}_{m\in \mathcal{L}_2} E((y-m(x))^2).\]</span>Now, if we have no further information on <span class="math inline">\(\mu\)</span> it might be quite hard to estimate it. Instead we can try to find a linear approximation to <span class="math inline">\(y\)</span> that uses the information we have, i.e.&nbsp;<span class="math inline">\(x\)</span>. A linear approximation using information on <span class="math inline">\(x\)</span> of <span class="math inline">\(y\)</span> has the following form: <span class="math display">\[\hat{y}=b_0+b_1x_1+\dots+b_px_p,\]</span> where <span class="math inline">\(b_0,\dots,b_p\in \mathbb{R}\)</span>. As before we want to find the linear approximation that minimizes the mean squared prediction error <span class="math display">\[E((y-b_0+b_1x_1+\dots+b_px_p)^2)=E((y-xb)), x=(1,x_1,\dots,x_p),\]</span> i.e.&nbsp;we find <span class="math inline">\(b=(b_0,\dots,b_p)^T\)</span> for which this expression is minimized (note: for <span class="math inline">\(E((y-xb)^2)\)</span> to be defined <span class="math inline">\(E(x_j^2)&lt;\infty\)</span> must hold for <span class="math inline">\(j=1,\dots,p\)</span>). Let’s rearrange <span class="math display">\[E((y-xb)^2)=E(y^2(-2E(yx)b+E((xb)^2 )\]</span> and take the derivative with respect to <span class="math inline">\(b\)</span>: <span class="math display">\[\frac{d}{db} E((y-xb)^2)=-2E(yx)+2(E(x_1x),\dots,E(x_px))^T b\]</span> Coming up with the last term is actually not straightforwards. Let’s recall what <span class="math inline">\(E((xb)^2)\)</span> actually is: <span class="math display">\[E((xb)^2)=\int_{\mathbb{R}^p} (xb)^2 p_{x_1,\dots,x_p}(x_1,\dots,x_p)dx_1\dots dx_p\]</span> Let’s consider <span class="math inline">\(j\in \{0,\dots,p\}\)</span> If everything goes sufficiently “smoothly” (actually in a mathematical sense) one can exchange integration and derivation, i.e.&nbsp;<span class="math display">\[\frac{\partial}{\partial b_j} E((b_0+b_1x_1+\dots+b_px_p)^2)= \int_{\mathbb{R}^p}\frac{\partial}{\partial b_j}(b_0+b_1x_1+\dots+b_px_p)^2 p(x)dx_1\dots dx_p =\\\int_{\mathbb{R}^p} 2(b_0+b_1x_1+\dots+b_px_p)x_jp(x)dx_1\dots dx_p=2E((b_0+b_1x_1+\dots +b_px_p)x_j),\]</span></p>
<p>with <span class="math inline">\(x_j=0\)</span>. Here we actually use that <span class="math inline">\(xbx_j\)</span> is bounded, which is related to the (co)variances of the <span class="math inline">\(x\)</span>’s. Generally speaking, this is one of these steps, where we just hope that everything behaves nicely. As I’m not an expert on stochastic calculus there might be results that guarantee that this actually works nicely in any case.</p>
<p>Now, we can set <span class="math display">\[\frac{d}{db} E((y-xb)^2)=0.\]</span> If we assume that the matrix <span class="math inline">\(E(x^tx)\)</span> is invertible we can solve for <span class="math inline">\(b\)</span> and get <span class="math display">\[\beta:=\hat{b}=E(x^T x))^{-1}E(yx).\]</span> Then, the linear projection of <span class="math inline">\(y\)</span> on the space spanned by <span class="math inline">\((1,x_1,\dots,x_p)\)</span> is <span class="math display">\[L(y|1,x_1,\dots,x_p)=x\beta.\]</span></p>
<p>The folowing is an interesting result: <span class="math inline">\(x\beta\)</span> is the linear projection of <span class="math inline">\(y\)</span> on <span class="math inline">\((1,x_1,\dots,x_p)\)</span> if and only if we can write <span class="math display">\[y=x\beta+u,\]</span> where the error term <span class="math inline">\(u\)</span> has “good” stochastic properties (<span class="math inline">\(E(u)=0, Cov(x_j,u)=0\)</span>). Let us show at least one direction:</p>
<p>Assume that <span class="math display">\[x\beta=\text{argmin}_{b\in \mathbb{R}^{p+1}}=E((y-xb)^2).\]</span> Then, <span class="math display">\[0=\frac{\partial}{\partial b_j} E((y-x\beta)=-2E((y-x\beta)x_j)=-2E(ux_j)\]</span> for <span class="math inline">\(j\in \{0,\dots,p\}\)</span>. Recall that <span class="math inline">\(x_0=1\)</span>. Hence, <span class="math inline">\(E(u)=0\)</span> and <span class="math inline">\(E(ux_j)=0\)</span>, which also implies <span class="math inline">\(Cov(x_j,u)=0\)</span>. To me this is quite surprising, as we get <span class="math inline">\(E(u)=0\)</span> without having to assume it.</p>
<p>Furthermore, the linear projection is the minimum MSE linear approximation to the conditional expectation function <span class="math inline">\(\mu(x)\)</span>. Hence, we could use our usual optimization toolkit to see that the linear projection is <span class="math display">\[\text{argmin}_{b} E[(\mu(x)-xb)^2].\]</span> It follows directly that, if <span class="math inline">\(\mu(x)\)</span> is linear, i.e.&nbsp;<span class="math inline">\(\mu(x)=xa\)</span> for <span class="math inline">\(a\in\mathbb{R}^{p+1}\)</span>, then the linear projection is the conditional expectation function.</p>
<p>The upshot of this discussion is: Instead of assuming <span class="math display">\[E(y|x_1,\dots,x_p)=\beta_0+\beta_1 x_1+\dots+\beta_px_p\]</span>we can just admit that the conditional expectation of <span class="math inline">\(y\)</span> is maybe more complicated and claim that we only estimate its linear approximation.</p>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-tschernigkursmaterial" class="csl-entry" role="listitem">
Tschernig, Rolf, and Harry Haupt. 2020. <span>“Kursmaterial f<span>ü</span>r Einf<span>ü</span>hrung in Die <span>Ö</span>konometrie.”</span>
</div>
<div id="ref-wooldridge2009introductory" class="csl-entry" role="listitem">
Wooldridge, Jeffrey Marc. 2009. <em>Introductory Econometrics: A Modern Approach</em>. ISE - International Student Edition. South-Western. <a href="http://books.google.ch/books?id=64vt5TDBNLwC">http://books.google.ch/books?id=64vt5TDBNLwC</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">© Copyright 2024, Florian Greindl</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">Built with <a href="https://quarto.org/">Quarto</a></div>
  </div>
</footer>



</body></html>